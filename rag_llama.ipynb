{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-19T07:07:25.488905Z",
     "start_time": "2025-06-19T07:07:19.886885Z"
    }
   },
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\SaaSProject\\chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:07:28.670157Z",
     "start_time": "2025-06-19T07:07:25.491916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# imports for langchain\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import requests"
   ],
   "id": "b1d35c08da9d5c72",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:07:29.366816Z",
     "start_time": "2025-06-19T07:07:29.363425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ],
   "id": "143ef58d463084df",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:07:29.384526Z",
     "start_time": "2025-06-19T07:07:29.373821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ],
   "id": "4c969035c0c9090",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:07:29.607701Z",
     "start_time": "2025-06-19T07:07:29.390854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "\n",
    "# With thanks to CG and Jon R, students on the course, for this fix needed for some users\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
    "# text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)"
   ],
   "id": "cf73d58c08927643",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:07:29.621842Z",
     "start_time": "2025-06-19T07:07:29.615761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)"
   ],
   "id": "778df1689f0601a0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:07:29.636329Z",
     "start_time": "2025-06-19T07:07:29.629867Z"
    }
   },
   "cell_type": "code",
   "source": "len(chunks)",
   "id": "18597049026f8825",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:07:29.650562Z",
     "start_time": "2025-06-19T07:07:29.646559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ],
   "id": "6484c32555f9823b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types found: employees, company, contracts, products\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:07:32.413567Z",
     "start_time": "2025-06-19T07:07:29.674932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "# BEFORE\n",
    "# vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "\n",
    "# AFTER\n",
    "vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "\n",
    "total_vectors = vectorstore.index.ntotal\n",
    "dimensions = vectorstore.index.d\n",
    "\n",
    "print(f\"There are {total_vectors} vectors with {dimensions:,} dimensions in the vector store\")"
   ],
   "id": "667cc8166ab5afe5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 123 vectors with 1,536 dimensions in the vector store\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:17:04.591925Z",
     "start_time": "2025-06-19T07:17:04.586926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def query_ollama(prompt, model):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    try:\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid JSON returned by Ollama: {response.text}\") from e\n",
    "\n",
    "    if \"response\" not in data:\n",
    "        raise ValueError(f\"Ollama returned unexpected response: {data}\")\n",
    "\n",
    "    return data[\"response\"]\n"
   ],
   "id": "18ce899424036c65",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:23:04.015294Z",
     "start_time": "2025-06-19T07:23:04.010753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.messages import AIMessage\n",
    "from typing import Union\n",
    "\n",
    "class LangchainOllamaRunnable(Runnable):\n",
    "    def __init__(self, model_name=\"tinyllama\"):\n",
    "        self.model_name = model_name\n",
    "    def invoke(self, input, *arg, **kwargs):\n",
    "        if isinstance(input, dict):\n",
    "            input = input.get(\"question\", \"\")\n",
    "        else:\n",
    "            input = str(input)\n",
    "\n",
    "        response = query_ollama(input, model=self.model_name)\n",
    "        return AIMessage(content=response)\n"
   ],
   "id": "ea503cb0df796140",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:15:11.558239Z",
     "start_time": "2025-06-19T07:15:11.554857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class OllamaRunnable(Runnable):\n",
    "    def __init__(self, model_name=\"llama3\"):\n",
    "        self.model = model_name\n",
    "\n",
    "    def invoke(self, input):\n",
    "        if isinstance(input, dict):\n",
    "            prompt = input.get(\"question\", \"\")\n",
    "        else:\n",
    "            prompt = str(input)\n",
    "\n",
    "        return {\"answer\": query_ollama(prompt, model=self.model)}\n"
   ],
   "id": "ff95fe9e2b6dd38d",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:23:06.830076Z",
     "start_time": "2025-06-19T07:23:06.826003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "USE_OLLAMA = True  # Set to False if you want to use OpenAI\n",
    "\n",
    "llm = LangchainOllamaRunnable(model_name=\"tinyllama\")\n"
   ],
   "id": "e435bd9caa9029db",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:23:07.117413Z",
     "start_time": "2025-06-19T07:23:07.107232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a new Chat with OpenAI\n",
    "# llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ],
   "id": "6934ae6191e8222b",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:23:34.777655Z",
     "start_time": "2025-06-19T07:23:30.843647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"Can you describe Insurellm in a few sentences\"\n",
    "conversation_chain.invoke({\n",
    "    \"question\": query,\n",
    "})"
   ],
   "id": "d443986f616f52e1",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "One input key expected got ['chat_history', 'question']",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[88]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m query = \u001B[33m\"\u001B[39m\u001B[33mCan you describe Insurellm in a few sentences\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mconversation_chain\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mquestion\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mchat_history\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\SaaSProject\\chatbot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:167\u001B[39m, in \u001B[36mChain.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    165\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    166\u001B[39m     run_manager.on_chain_error(e)\n\u001B[32m--> \u001B[39m\u001B[32m167\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m    168\u001B[39m run_manager.on_chain_end(outputs)\n\u001B[32m    170\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\SaaSProject\\chatbot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:162\u001B[39m, in \u001B[36mChain.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    155\u001B[39m     \u001B[38;5;28mself\u001B[39m._validate_inputs(inputs)\n\u001B[32m    156\u001B[39m     outputs = (\n\u001B[32m    157\u001B[39m         \u001B[38;5;28mself\u001B[39m._call(inputs, run_manager=run_manager)\n\u001B[32m    158\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m    159\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call(inputs)\n\u001B[32m    160\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m162\u001B[39m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mprep_outputs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    163\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\n\u001B[32m    164\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    165\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    166\u001B[39m     run_manager.on_chain_error(e)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\SaaSProject\\chatbot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:463\u001B[39m, in \u001B[36mChain.prep_outputs\u001B[39m\u001B[34m(self, inputs, outputs, return_only_outputs)\u001B[39m\n\u001B[32m    461\u001B[39m \u001B[38;5;28mself\u001B[39m._validate_outputs(outputs)\n\u001B[32m    462\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.memory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m463\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m.\u001B[49m\u001B[43msave_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    464\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m return_only_outputs:\n\u001B[32m    465\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\SaaSProject\\chatbot\\.venv\\Lib\\site-packages\\langchain\\memory\\chat_memory.py:72\u001B[39m, in \u001B[36mBaseChatMemory.save_context\u001B[39m\u001B[34m(self, inputs, outputs)\u001B[39m\n\u001B[32m     70\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34msave_context\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any], outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     71\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m72\u001B[39m     input_str, output_str = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_input_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     73\u001B[39m     \u001B[38;5;28mself\u001B[39m.chat_memory.add_messages(\n\u001B[32m     74\u001B[39m         [\n\u001B[32m     75\u001B[39m             HumanMessage(content=input_str),\n\u001B[32m     76\u001B[39m             AIMessage(content=output_str),\n\u001B[32m     77\u001B[39m         ]\n\u001B[32m     78\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\SaaSProject\\chatbot\\.venv\\Lib\\site-packages\\langchain\\memory\\chat_memory.py:47\u001B[39m, in \u001B[36mBaseChatMemory._get_input_output\u001B[39m\u001B[34m(self, inputs, outputs)\u001B[39m\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_get_input_output\u001B[39m(\n\u001B[32m     44\u001B[39m     \u001B[38;5;28mself\u001B[39m, inputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any], outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]\n\u001B[32m     45\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[32m     46\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.input_key \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m47\u001B[39m         prompt_input_key = \u001B[43mget_prompt_input_key\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmemory_variables\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     48\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     49\u001B[39m         prompt_input_key = \u001B[38;5;28mself\u001B[39m.input_key\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\SaaSProject\\chatbot\\.venv\\Lib\\site-packages\\langchain\\memory\\utils.py:19\u001B[39m, in \u001B[36mget_prompt_input_key\u001B[39m\u001B[34m(inputs, memory_variables)\u001B[39m\n\u001B[32m     17\u001B[39m prompt_input_keys = \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mset\u001B[39m(inputs).difference(memory_variables + [\u001B[33m\"\u001B[39m\u001B[33mstop\u001B[39m\u001B[33m\"\u001B[39m]))\n\u001B[32m     18\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(prompt_input_keys) != \u001B[32m1\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mOne input key expected got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprompt_input_keys\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m prompt_input_keys[\u001B[32m0\u001B[39m]\n",
      "\u001B[31mValueError\u001B[39m: One input key expected got ['chat_history', 'question']"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:23:46.953197Z",
     "start_time": "2025-06-19T07:23:46.949197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Wrapping that in a function\n",
    "\n",
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\n",
    "        \"question\": message,\n",
    "        \"chat_history\": history or []\n",
    "    })\n",
    "    return result[\"answer\"]\n"
   ],
   "id": "feb6a893d809a1c3",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:23:50.139381Z",
     "start_time": "2025-06-19T07:23:49.223272Z"
    }
   },
   "cell_type": "code",
   "source": "view = gr.ChatInterface(chat).launch(inbrowser=True)",
   "id": "38f20b8b96663420",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\SaaSProject\\chatbot\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 90
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
